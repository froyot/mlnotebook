{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python 文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据实用复旦中文文本分类数据集,[下载地址](https://pan.baidu.com/s/1833mT2rhL6gBMlM0KnmyKg) 密码:zyxa\n",
    "\n",
    "文本分类步骤:\n",
    "\n",
    "1) 对每篇文档进行分词，生成空格分隔的文档\n",
    "\n",
    "2) 计算每篇文档TF-IDF矩阵，fit_transform\n",
    "\n",
    "3) 训练模型，以tf-idf矩阵为特征，文档分类为label，使用贝叶斯或其他分类算法，训练模型\n",
    "\n",
    "4) 计算测试数据tf-idf值。transform（test.contents）\n",
    "\n",
    "5) 使用测试数据tf-idf 值，进行预测mode.predict(test.tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF（Term Frequency-Inverse Document Frequency）即词频-逆文档频率，一般用在文本描述中。\n",
    "\n",
    "主要思想是通过统计文章的关键词频率，来衡量和某个主题的相近程度或者计算文章之间的相似性。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['安徽', '落实', '落实', '发展', '城市', '建设']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "title = \"安徽落实落实发展城市建设\"\n",
    "words = jieba.cut(title)\n",
    "words = list(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "安徽 0.3816141458138271\n",
      "落实 0.6461289150464732\n",
      "发展 0.3816141458138271\n",
      "城市 0.3816141458138271\n",
      "建设 0.3816141458138271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "change_title = \" \".join(words)\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=1,min_df=0)\n",
    "x = vectorizer.fit_transform([change_title])\n",
    "\n",
    "for key,value in vectorizer.vocabulary_.items():\n",
    "    print(key,x[0,value])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算步骤\n",
    "\n",
    "1)计算TF(单词在单个文章或句子的出现评率)\n",
    "\n",
    "TF=单词出现次数/总单词个数\n",
    "\n",
    "2)计算IDF（计算单词的重要程度）\n",
    "\n",
    "IDF = log(文章总数/(包含单词文章数+1))\n",
    "\n",
    "这个公式体现了一个意思，那就是说如果某个词在所有的语句、文章中都经常出现\n",
    "\n",
    "那么说明它就是个常用词，不重要，那么计算出的IDF就会很小（PS:+1是为了防止除以0，万一有的词从未出现过）\n",
    "\n",
    "3)TF-IDF\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "\n",
    "\n",
    "sklearn中tf idf计算有些改变\n",
    "\n",
    "如果参数sublinear_tf=True,则tf = 1+ln(tf)\n",
    "tf = 单词在文档出现次数/1\n",
    "\n",
    "idf = ln((文章总数+smooth)/(包含文章数+smooth))+1 ，smooth=1\n",
    "\n",
    "默认tf-idf返回前会进行归一化，使用L2正则化\n",
    "tf-idf = norm ( tf * idf )\n",
    "\n",
    "上述例子中\n",
    "\"落实\" tf=1+ln(2) =1+0.6931471806  =1.6931471806\n",
    "IDF = log((1+1)/(1+1))+1 = 1 ，smooth=1\n",
    "tf-idf = norm(1.6931471806)\n",
    "\n",
    "归一化 1.6931471806/(1.6931471806^2 +1^2+1^2+1^2+1^2)^0.5 = 1.6931471806 / 2.62044793407038122 = 0.646128915055377\n",
    "\n",
    "\n",
    " vectorizer.fit_transform 返回一个矩阵,每一行表示一个句子或文档。每列表示一个单词的tf-idf值\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词采用jieba分词\n",
    "\n",
    "分词后，把分词结果保存到文件，避免重复分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os  \n",
    "import jieba\n",
    "import re\n",
    "\n",
    "\n",
    "datasetdir = \"D:\\my\\ml\\data\\ch_document_category\"\n",
    "changedatadir = \"D:\\my\\ml\\data\\changedatadir\"\n",
    "stopwordpath = \"D:\\my\\ml\\data\\ch_stopword.txt\"\n",
    "stopwords = [line.strip() for line in open(stopwordpath, 'r', encoding='utf-8').readlines()]  \n",
    "## 数据格式，每个目录表示一个分类\n",
    "## 文本数据开头有一些作者，标题，sn号等信息\n",
    "## 对每篇文章分词前进行预处理，删除空格，换行，sn号，出版日期等\n",
    "\n",
    "def pre_handler_content(content):\n",
    "    patten = \"【 标  题 】(.*?)\\\\r?\\\\n【 正  文 】([\\s\\S]*)\"\n",
    "    match = re.search(patten,content, re.M|re.I)\n",
    "    if match != None:\n",
    "        g = match.groups()\n",
    "        if len(g) == 2:\n",
    "            content = \"\".join(g)\n",
    "    content = re.sub(\"[\\r\\n\\t\\s＊]\", \"\", content)\n",
    "    return content\n",
    "    \n",
    "    \n",
    "def split_world_and_tostr(content):\n",
    "    words = jieba.cut(content)\n",
    "    savewords = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            savewords.append(word)\n",
    "    if len(savewords)>0:\n",
    "        return \" \".join(savewords)\n",
    "    return ''\n",
    "\n",
    "\n",
    "## 获取子目录\n",
    "catelist = os.listdir(datasetdir)\n",
    "\n",
    "\n",
    "def makesplit():\n",
    "    for dirname in catelist:\n",
    "        fulldirname = os.path.join(datasetdir,dirname)\n",
    "        newdirname = os.path.join(changedatadir,dirname)\n",
    "        if not os.path.exists(newdirname):  # 是否存在分词目录，如果没有则创建该目录  \n",
    "            os.makedirs(newdirname) \n",
    "        file_list = os.listdir(fulldirname)\n",
    "        for filename in file_list:\n",
    "            change_str = \"\"\n",
    "            with open(os.path.join(fulldirname,filename), \"rb\") as f:\n",
    "                content = f.read()\n",
    "                if content!= None:\n",
    "                    content = content.decode('gbk', 'ignore')\n",
    "                    content = pre_handler_content(content)\n",
    "                    change_str = split_world_and_tostr(content)\n",
    "            if len(change_str)>0:\n",
    "                with open(os.path.join(newdirname,filename),\"w+\") as f:\n",
    "                    f.write(change_str)\n",
    "\n",
    "    \n",
    "\n",
    "makesplit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结构化表示-构建词向量空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import base\n",
    "import random\n",
    "\n",
    "def load_data_to_learn():\n",
    "    bunchtrain = base.Bunch(target_name=[], label=[], filenames=[], contents=[])\n",
    "    bunchtest = base.Bunch(target_name=[], label=[], filenames=[], contents=[])\n",
    "    catelist = os.listdir(changedatadir)\n",
    "    for dirname in catelist:\n",
    "        fulldirname = os.path.join(changedatadir,dirname)\n",
    "        file_list = os.listdir(fulldirname)\n",
    "        totalcount = len(file_list)\n",
    "        trainnum = 0\n",
    "        \n",
    "        for filename in file_list:\n",
    "            fullfilename = os.path.join(fulldirname,filename)\n",
    "            with open(fullfilename, \"rb\") as f:\n",
    "                content = f.read()\n",
    "                if content!= None:\n",
    "                    content = content.decode('gbk', 'ignore')\n",
    "                    r = random.random()\n",
    "                    if r<0.7:\n",
    "                        bunchtrain.label.append(dirname)\n",
    "                        bunchtrain.target_name.append(dirname)\n",
    "                        bunchtrain.filenames.append(fullfilename)\n",
    "                        bunchtrain.contents.append(content)\n",
    "                    else:\n",
    "                        bunchtest.label.append(dirname)\n",
    "                        bunchtest.target_name.append(dirname)\n",
    "                        bunchtest.filenames.append(fullfilename)\n",
    "                        bunchtest.contents.append(content)\n",
    "\n",
    "    return (bunchtrain,bunchtest)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建训练集文本对象结束！！！\n",
      "构建训练集文本对象结束！！！\n"
     ]
    }
   ],
   "source": [
    "import _pickle as cPickle\n",
    "\n",
    "bunch,bunchtest = load_data_to_learn()\n",
    "\n",
    "rumtimepath = \"D:\\\\my\\\\ml\\\\data\\\\runtime\"\n",
    "with open(os.path.join(rumtimepath,\"bunch.dat\"), \"wb+\") as file_obj:  \n",
    "    cPickle.dump(bunch, file_obj)\n",
    "    print(\"构建训练集文本对象结束！！！\")\n",
    "\n",
    "with open(os.path.join(rumtimepath,\"bunchtest.dat\"), \"wb+\") as file_obj:  \n",
    "    cPickle.dump(bunchtest, file_obj)\n",
    "    print(\"构建训练集文本对象结束！！！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf 构建结束！！！\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfspace = base.Bunch(target_name=bunch.target_name, label=bunch.label, filenames=bunch.filenames, tdm=[], vocabulary={})\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n",
    "tfidfspace.tdm = vectorizer.fit_transform(bunch.contents)\n",
    "tfidfspace.vocabulary = vectorizer.vocabulary_\n",
    "\n",
    "\n",
    "\n",
    "with open(os.path.join(rumtimepath,\"tfidfspace.dat\"), \"wb+\") as file_obj:  \n",
    "    cPickle.dump(tfidfspace, file_obj)  \n",
    "    print(\"tfidf 构建结束！！！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = os.path.join(rumtimepath,\"feture.dat\")\n",
    "with open(feature_path, 'wb') as fw:\n",
    "    cPickle.dump(vectorizer.vocabulary_, fw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.feature_extraction.text import CountVectorizer\n",
    "loaded_vec = CountVectorizer(decode_error=\"replace\", vocabulary=cPickle.load(open(feature_path, \"rb\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "\n",
    "### 分割测试，训练\n",
    "traindata_x = tfidfspace.tdm\n",
    "traindata_y = bunch.label\n",
    "\n",
    "\n",
    "testdata_x = vectorizer.transform(bunchtest.contents)\n",
    "testdata_y = bunchtest.label\n",
    "# countnum = len(tfidfspace.label)\n",
    "# for i in range(0,countnum-1):\n",
    "#     r = random.random()\n",
    "#     if r<0.3:\n",
    "#         testdata_x = tfidfspace.tdm\n",
    "#         testdata_y = tfidfspace.label\n",
    "#     else:\n",
    "#         traindata_x = tfidfspace.tdm\n",
    "#         traindata_y = tfidfspace.label\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉验证结果: [0.73417722 0.79220779 0.73611111 0.81944444 0.81428571 0.82089552\n",
      " 0.78125    0.7704918  0.7704918  0.78333333]\n",
      "交叉验证结果: [0.75949367 0.79220779 0.75       0.84722222 0.85714286 0.82089552\n",
      " 0.796875   0.78688525 0.73770492 0.76666667]\n",
      "交叉验证结果: [0.75949367 0.79220779 0.76388889 0.84722222 0.85714286 0.80597015\n",
      " 0.796875   0.78688525 0.75409836 0.75      ]\n",
      "交叉验证结果: [0.75949367 0.80519481 0.77777778 0.80555556 0.77142857 0.80597015\n",
      " 0.8125     0.78688525 0.7704918  0.76666667]\n"
     ]
    }
   ],
   "source": [
    "alpha_list = [0.00035,0.0006,0.001,0.01]\n",
    "for al in alpha_list:\n",
    "    clf = MultinomialNB(alpha=al)                                  \n",
    "    accs=cross_val_score(clf,traindata_x ,traindata_y,cv=10,scoring='accuracy')\n",
    "    print('交叉验证结果:',accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.0006, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.0006)\n",
    "clf.fit(traindata_x ,traindata_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\my\\ml\\data\\changedatadir\\C11-Space\\C11-Space0086.txt 实际分类：C11-Space -- 预测结果:C39-Sports\n",
      "D:\\my\\ml\\data\\changedatadir\\C16-Electronics\\C16-Electronics10.txt 实际分类：C16-Electronics -- 预测结果:C32-Agriculture\n",
      "D:\\my\\ml\\data\\changedatadir\\C16-Electronics\\C16-Electronics20.txt 实际分类：C16-Electronics -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C16-Electronics\\C16-Electronics22.txt 实际分类：C16-Electronics -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C16-Electronics\\C16-Electronics30.txt 实际分类：C16-Electronics -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C16-Electronics\\C16-Electronics38.txt 实际分类：C16-Electronics -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C16-Electronics\\C16-Electronics47.txt 实际分类：C16-Electronics -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C16-Electronics\\C16-Electronics55.txt 实际分类：C16-Electronics -- 预测结果:C11-Space\n",
      "D:\\my\\ml\\data\\changedatadir\\C17-Communication\\C17-Communication19.txt 实际分类：C17-Communication -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C17-Communication\\C17-Communication25.txt 实际分类：C17-Communication -- 预测结果:C29-Transport\n",
      "D:\\my\\ml\\data\\changedatadir\\C17-Communication\\C17-Communication51.txt 实际分类：C17-Communication -- 预测结果:C19-Computer\n",
      "D:\\my\\ml\\data\\changedatadir\\C23-Mine\\C23-Mine48.txt 实际分类：C23-Mine -- 预测结果:C31-Enviornment\n",
      "D:\\my\\ml\\data\\changedatadir\\C23-Mine\\C23-Mine50.txt 实际分类：C23-Mine -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C23-Mine\\C23-Mine54.txt 实际分类：C23-Mine -- 预测结果:C32-Agriculture\n",
      "D:\\my\\ml\\data\\changedatadir\\C23-Mine\\C23-Mine65.txt 实际分类：C23-Mine -- 预测结果:C29-Transport\n",
      "D:\\my\\ml\\data\\changedatadir\\C29-Transport\\C29-Transport017.txt 实际分类：C29-Transport -- 预测结果:C11-Space\n",
      "D:\\my\\ml\\data\\changedatadir\\C29-Transport\\C29-Transport019.txt 实际分类：C29-Transport -- 预测结果:C11-Space\n",
      "D:\\my\\ml\\data\\changedatadir\\C34-Economy\\C34-Economy0065.txt 实际分类：C34-Economy -- 预测结果:C32-Agriculture\n",
      "D:\\my\\ml\\data\\changedatadir\\C34-Economy\\C34-Economy0084.txt 实际分类：C34-Economy -- 预测结果:C7-History\n",
      "D:\\my\\ml\\data\\changedatadir\\C36-Medical\\C36-Medical004.txt 实际分类：C36-Medical -- 预测结果:C39-Sports\n",
      "D:\\my\\ml\\data\\changedatadir\\C36-Medical\\C36-Medical012.txt 实际分类：C36-Medical -- 预测结果:C15-Energy\n",
      "D:\\my\\ml\\data\\changedatadir\\C36-Medical\\C36-Medical040.txt 实际分类：C36-Medical -- 预测结果:C39-Sports\n",
      "D:\\my\\ml\\data\\changedatadir\\C37-Military\\C37-Military015.txt 实际分类：C37-Military -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C37-Military\\C37-Military019.txt 实际分类：C37-Military -- 预测结果:C5-Education\n",
      "D:\\my\\ml\\data\\changedatadir\\C37-Military\\C37-Military042.txt 实际分类：C37-Military -- 预测结果:C5-Education\n",
      "D:\\my\\ml\\data\\changedatadir\\C37-Military\\C37-Military149.txt 实际分类：C37-Military -- 预测结果:C5-Education\n",
      "D:\\my\\ml\\data\\changedatadir\\C38-Politics\\C38-Politics0032.txt 实际分类：C38-Politics -- 预测结果:C6-Philosophy\n",
      "D:\\my\\ml\\data\\changedatadir\\C38-Politics\\C38-Politics0034.txt 实际分类：C38-Politics -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C38-Politics\\C38-Politics0038.txt 实际分类：C38-Politics -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C39-Sports\\C39-Sports0028.txt 实际分类：C39-Sports -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C39-Sports\\C39-Sports0183.txt 实际分类：C39-Sports -- 预测结果:C7-History\n",
      "D:\\my\\ml\\data\\changedatadir\\C39-Sports\\C39-Sports0191.txt 实际分类：C39-Sports -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C39-Sports\\C39-Sports0203.txt 实际分类：C39-Sports -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C4-Literature\\C4-Literature09.txt 实际分类：C4-Literature -- 预测结果:C5-Education\n",
      "D:\\my\\ml\\data\\changedatadir\\C4-Literature\\C4-Literature33.txt 实际分类：C4-Literature -- 预测结果:C7-History\n",
      "D:\\my\\ml\\data\\changedatadir\\C4-Literature\\C4-Literature50.txt 实际分类：C4-Literature -- 预测结果:C5-Education\n",
      "D:\\my\\ml\\data\\changedatadir\\C4-Literature\\C4-Literature52.txt 实际分类：C4-Literature -- 预测结果:C3-Art\n",
      "D:\\my\\ml\\data\\changedatadir\\C4-Literature\\C4-Literature60.txt 实际分类：C4-Literature -- 预测结果:C5-Education\n",
      "D:\\my\\ml\\data\\changedatadir\\C4-Literature\\C4-Literature62.txt 实际分类：C4-Literature -- 预测结果:C5-Education\n",
      "D:\\my\\ml\\data\\changedatadir\\C4-Literature\\C4-Literature66.txt 实际分类：C4-Literature -- 预测结果:C7-History\n",
      "D:\\my\\ml\\data\\changedatadir\\C5-Education\\C5-Education002.txt 实际分类：C5-Education -- 预测结果:C3-Art\n",
      "D:\\my\\ml\\data\\changedatadir\\C5-Education\\C5-Education064.txt 实际分类：C5-Education -- 预测结果:C39-Sports\n",
      "D:\\my\\ml\\data\\changedatadir\\C5-Education\\C5-Education068.txt 实际分类：C5-Education -- 预测结果:C36-Medical\n",
      "D:\\my\\ml\\data\\changedatadir\\C6-Philosophy\\C6-Philosophy40.txt 实际分类：C6-Philosophy -- 预测结果:C7-History\n",
      "D:\\my\\ml\\data\\changedatadir\\C6-Philosophy\\C6-Philosophy48.txt 实际分类：C6-Philosophy -- 预测结果:C7-History\n",
      "D:\\my\\ml\\data\\changedatadir\\C6-Philosophy\\C6-Philosophy50.txt 实际分类：C6-Philosophy -- 预测结果:C3-Art\n",
      "D:\\my\\ml\\data\\changedatadir\\C6-Philosophy\\C6-Philosophy75.txt 实际分类：C6-Philosophy -- 预测结果:C5-Education\n",
      "D:\\my\\ml\\data\\changedatadir\\C6-Philosophy\\C6-Philosophy79.txt 实际分类：C6-Philosophy -- 预测结果:C5-Education\n",
      "D:\\my\\ml\\data\\changedatadir\\C6-Philosophy\\C6-Philosophy85.txt 实际分类：C6-Philosophy -- 预测结果:C5-Education\n",
      "D:\\my\\ml\\data\\changedatadir\\C6-Philosophy\\C6-Philosophy87.txt 实际分类：C6-Philosophy -- 预测结果:C5-Education\n",
      "D:\\my\\ml\\data\\changedatadir\\C7-History\\C7-History020.txt 实际分类：C7-History -- 预测结果:C34-Economy\n",
      "D:\\my\\ml\\data\\changedatadir\\C7-History\\C7-History046.txt 实际分类：C7-History -- 预测结果:C3-Art\n",
      "D:\\my\\ml\\data\\changedatadir\\C7-History\\C7-History061.txt 实际分类：C7-History -- 预测结果:C3-Art\n",
      "预测结束 测试集总共306 ，分类错误 53\n",
      "精度:0.863814\n",
      "召回:0.826797\n",
      "f1-score:0.821827\n"
     ]
    }
   ],
   "source": [
    "## 预测结果\n",
    "\n",
    "predicted = clf.predict(testdata_x)\n",
    "\n",
    "count = 0\n",
    "for flabel,filename,expct_cate in zip(testdata_y,bunchtest.filenames,predicted):\n",
    "    if flabel != expct_cate:\n",
    "        count = count+1\n",
    "        print(\"%s 实际分类：%s -- 预测结果:%s\" % (filename,flabel,expct_cate))\n",
    "print(\"预测结束 测试集总共%d ，分类错误 %d\" % (len(testdata_y),count))\n",
    " \n",
    "# 计算分类精度：\n",
    "from sklearn import metrics\n",
    "def metrics_result(actual, predict):\n",
    "    print(\"精度:%3f\" % metrics.precision_score(actual, predict,average='weighted'))\n",
    "    print(\"召回:%3f\" % metrics.recall_score(actual, predict,average='weighted'))\n",
    "    print(\"f1-score:%3f\" % metrics.f1_score(actual, predict,average='weighted'))\n",
    "\n",
    "metrics_result(testdata_y, predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
