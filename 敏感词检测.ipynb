{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python 敏感词检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45603\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.datasets import base\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "\n",
    "stopworddir = \"D:\\my\\ml\\data\\ch_stopword.txt\"\n",
    "\n",
    "stopwords = [line.strip() for line in open(stopworddir, 'r', encoding='utf-8').readlines()]  \n",
    "\n",
    "normaldatatxt = \"D:\\my\\ml\\data\\\\centens_word\\\\晚章.txt\"\n",
    "normaldatadirtxt = \"D:\\my\\ml\\data\\\\centens_word\\\\正常.txt\"\n",
    "content = ''\n",
    "with open(normaldatatxt) as f:\n",
    "    content = f.read()\n",
    "    content = re.sub(\"[\\s\\r\\n]\",\"\",content)\n",
    "\n",
    "if len(content)>0:\n",
    "    words = jieba.cut(content)\n",
    "\n",
    "    change_list = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            change_list.append(word)\n",
    "    change_str = \"\\n\".join(list(set(change_list)))\n",
    "    print(len(change_str))\n",
    "    with open(normaldatadirtxt,\"w+\") as f:\n",
    "        f.write(change_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from sklearn.datasets import base\n",
    "import jieba\n",
    "\n",
    "def loaddata():\n",
    "    stopworddir = \"D:\\my\\ml\\data\\centens_word\"\n",
    "\n",
    "    filenames = os.listdir(stopworddir)\n",
    "    ## 获取所有分类\n",
    "    train_data = base.Bunch(label=[],contents=[],tfidf=[])\n",
    "\n",
    "    test_data = base.Bunch(label=[],contents=[],tfidf=[])\n",
    "\n",
    "    for filename in filenames:\n",
    "        \n",
    "        with open(os.path.join(stopworddir,filename)) as f:\n",
    "            content = f.read()\n",
    "            content = content.replace(\"\\r\",\"\\n\")\n",
    "            content = content.replace(\"\\n\\n\",\"\\n\")\n",
    "            lines = content.split(\"\\n\")\n",
    "            n = 0\n",
    "            l = len(lines)\n",
    "            ## 分类不平衡，对数据少的类别进行重复采样\n",
    "            while (l<14000 and n<10) or l>14000:\n",
    "                extendlabel = [filename]*l\n",
    "                random.shuffle(lines)\n",
    "                train_data.contents.extend(lines)\n",
    "                train_data.label.extend(extendlabel)\n",
    "                n = n+1\n",
    "                if l>14000:\n",
    "                    break\n",
    "                \n",
    "    return train_data\n",
    "    \n",
    "\n",
    "train_data = loaddata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38680\n",
      "38680\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data.label))\n",
    "print(len(train_data.contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25915, 14992), (12765, 14992)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "f = CountVectorizer()\n",
    "X = f.fit_transform(train_data.contents)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, train_data.label, test_size=0.33, random_state=42)\n",
    "print([np.shape(X_train), np.shape(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交叉验证结果: [0.98920419 0.99167181 0.98581123 0.99012041 0.98981167 0.99166152\n",
      " 0.99382144 0.98640297]\n",
      "交叉验证结果: [0.98766194 0.99167181 0.98735349 0.98950293 0.99012041 0.99104385\n",
      " 0.99382144 0.98702101]\n",
      "交叉验证结果: [0.9605182  0.96360271 0.96082665 0.95986416 0.96326027 0.95985176\n",
      " 0.96200185 0.96260816]\n",
      "交叉验证结果: [0.89265885 0.8923504  0.89481801 0.89286817 0.89626428 0.89314392\n",
      " 0.89620019 0.89091471]\n",
      "交叉验证结果: [0.76465145 0.75663171 0.7553979  0.76072862 0.76134609 0.75200741\n",
      " 0.76367006 0.74814586]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "alpha_list = [0.00001,0.1,1,2,4]\n",
    "\n",
    "for al in alpha_list:\n",
    "    clf = MultinomialNB(alpha=al)                                  \n",
    "    accs=cross_val_score(clf,X_train ,y_train,cv=8,scoring='accuracy')\n",
    "    print('交叉验证结果:',accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12765,)\n",
      "预测结束 测试集总共12765 ，分类错误 131\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.00001)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "test_predict = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(test_predict.shape)\n",
    "count = 0\n",
    "for flabel,expct_cate in zip(y_test,test_predict[:]):\n",
    "    if flabel != expct_cate:\n",
    "        count = count+1\n",
    "\n",
    "print(\"预测结束 测试集总共%d ，分类错误 %d\" % (len(y_test),count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12765, 14992)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['正常.txt' '正常.txt' '正常.txt' '正常.txt' '民生词库.txt' '正常.txt']\n",
      "['全球化', '进程', '加快', '，', '盗窃', '猖獗']\n"
     ]
    }
   ],
   "source": [
    "teststr = jieba.cut(\"全球化进程加快，盗窃猖獗\")\n",
    "words = list(teststr)\n",
    "x_teststr = f.transform(words)\n",
    "print(clf.predict(x_teststr))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
